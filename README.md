# Brazilian E-commerce Analytics Pipeline

[![Java](https://img.shields.io/badge/Java-11-orange.svg)](https://openjdk.java.net/)
[![Spark](https://img.shields.io/badge/Apache%20Spark-3.4.0-red.svg)](https://spark.apache.org/)
[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)]()

A **production-grade batch ETL pipeline** implementing the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) for Brazilian e-commerce data analysis using Apache Spark and Java.

## ğŸ¯ Project Objectives

This project demonstrates enterprise-level data engineering practices including:
- **Medallion Architecture** (Bronze/Silver/Gold layers)
- **Data Quality Management** with validation rules and dead letter queues
- **Idempotent Processing** for reliable reruns
- **Schema Evolution** and type-safe data modeling
- **Comprehensive Logging** and observability
- **Configuration Management** for multiple environments

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ RAW LAYER (CSV Files)
   â””â”€â”€ Brazilian E-commerce Dataset (Kaggle)
       â”œâ”€â”€ Orders
       â”œâ”€â”€ Customers  
       â”œâ”€â”€ Products
       â””â”€â”€ Order Items
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‰ BRONZE LAYER (Minimal Transformation)                       â”‚
â”‚  - Schema validation                                             â”‚
â”‚  - Raw data ingestion                                            â”‚
â”‚  - Parquet format                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥ˆ SILVER LAYER (Cleaned & Validated)                          â”‚
â”‚  - Data Quality Checks âœ“                                         â”‚
â”‚  - Deduplication                                                 â”‚
â”‚  - Null Handling                                                 â”‚
â”‚  - Data Enrichment (calculated fields)                           â”‚
â”‚  - Dead Letter Queue for bad records                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¥‡ GOLD LAYER (Business Aggregates) - Coming Soon!             â”‚
â”‚  - Dimensional modeling                                          â”‚
â”‚  - Pre-aggregated metrics                                        â”‚
â”‚  - Ready for BI tools                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Features Implemented

### âœ… Phase 1: Data Ingestion (Bronze Layer)
- [x] **Schema-first approach** with explicit type definitions
- [x] **Configurable data readers** for multiple file formats
- [x] **Error handling** with corrupt record capture
- [x] **Data validation** at ingestion time
- [x] **Environment-specific configurations** (dev/prod)

### âœ… Phase 2: Data Transformation (Silver Layer)
- [x] **Data Quality Rules Engine**
  - Null validation for primary keys
  - Order status validation
  - Logical date validations
  - Business rule checks
- [x] **Dead Letter Queue Pattern** - Quarantines invalid records
- [x] **Deduplication Logic** - Keeps latest records by timestamp
- [x] **Data Enrichment**
  - Delivery time calculations
  - Late delivery flags
  - Order approval time
  - Date dimension extraction (year, month, quarter, day of week)
- [x] **Idempotent Processing** - Safe to rerun multiple times
- [x] **Partitioning Strategy** - Optimized for query performance

### ğŸ”œ Phase 3: Analytics Layer (Gold Layer) - Coming Soon
- [ ] Dimensional modeling (Fact & Dimension tables)
- [ ] Pre-aggregated business metrics
- [ ] Customer segmentation
- [ ] Product performance analysis
- [ ] Sales trends and seasonality

---

## ğŸ“Š Data Quality Framework

### Validation Rules Implemented

| Rule | Layer | Description | Action |
|------|-------|-------------|--------|
| **Null Primary Key** | Silver | Checks `order_id`, `customer_id` are not null | Quarantine |
| **Invalid Status** | Silver | Validates order status is in allowed list | Quarantine |
| **Null Timestamps** | Silver | Ensures `purchase_timestamp` is not null | Quarantine |
| **Logical Dates** | Silver | Validates delivery date â‰¥ purchase date | Quarantine |
| **Duplicates** | Silver | Removes duplicate `order_id`, keeps latest | Auto-fix |

### Dead Letter Queue (Quarantine)

Bad records are not lost! They are:
- Written to `/data/quarantine/` with reasons
- Partitioned by processing date
- Available for investigation and reprocessing
- Include original data + validation failure reasons

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Language** | Java | 11 |
| **Processing Engine** | Apache Spark | 3.4.0 |
| **Build Tool** | Maven | 3.8+ |
| **Logging** | SLF4J + Log4j2 | 2.20.0 |
| **Configuration** | Typesafe Config | 1.4.2 |
| **Testing** | JUnit + Mockito | 4.13.2 / 5.3.1 |

---

## ğŸ“ Project Structure

```
brazilian-ecommerce-analytics/
â”œâ”€â”€ src/main/java/com/ecommerce/
â”‚   â”œâ”€â”€ config/                    # Configuration management
â”‚   â”‚   â”œâ”€â”€ AppConfig.java         # Centralized config loader
â”‚   â”‚   â””â”€â”€ SparkSessionFactory.java
â”‚   â”œâ”€â”€ schema/                    # Schema definitions
â”‚   â”‚   â”œâ”€â”€ OrderSchema.java
â”‚   â”‚   â”œâ”€â”€ CustomerSchema.java
â”‚   â”‚   â””â”€â”€ ProductSchema.java
â”‚   â”œâ”€â”€ models/                    # Data models (POJOs)
â”‚   â”‚   â”œâ”€â”€ Order.java
â”‚   â”‚   â”œâ”€â”€ Customer.java
â”‚   â”‚   â””â”€â”€ Product.java
â”‚   â”œâ”€â”€ ingestion/                 # Data ingestion (Bronze)
â”‚   â”‚   â”œâ”€â”€ DataReader.java
â”‚   â”‚   â”œâ”€â”€ DataValidator.java
â”‚   â”‚   â””â”€â”€ DatasetDownloader.java
â”‚   â”œâ”€â”€ transformation/            # Data transformation
â”‚   â”‚   â””â”€â”€ silver/
â”‚   â”‚       â””â”€â”€ OrdersSilverTransformer.java
â”‚   â””â”€â”€ jobs/                      # Spark job entry points
â”‚       â”œâ”€â”€ DataIngestionJob.java
â”‚       â”œâ”€â”€ SilverTransformationJob.java
â”‚       â””â”€â”€ SimpleTestJob.java
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.conf           # Base configuration
â”‚   â”œâ”€â”€ application-dev.conf       # Dev overrides
â”‚   â”œâ”€â”€ application-prod.conf      # Prod overrides
â”‚   â””â”€â”€ log4j2.xml                 # Logging configuration
â”œâ”€â”€ data/                          # Data storage (gitignored)
â”‚   â”œâ”€â”€ raw/                       # Raw CSV files
â”‚   â”œâ”€â”€ staging/                   # Bronze layer (parquet)
â”‚   â”œâ”€â”€ silver/                    # Silver layer (cleaned)
â”‚   â”œâ”€â”€ gold/                      # Gold layer (aggregates)
â”‚   â””â”€â”€ quarantine/                # Bad records (DLQ)
â”œâ”€â”€ logs/                          # Application logs
â”œâ”€â”€ pom.xml                        # Maven dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸš¦ Getting Started

### Prerequisites
- Java 11 or higher
- Maven 3.8+
- 4GB+ RAM recommended

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd brazilian-ecommerce-analytics

# Build the project
mvn clean package

# Run ingestion job (Bronze layer)
mvn exec:java -Dexec.mainClass="com.ecommerce.jobs.DataIngestionJob"

# Run transformation job (Silver layer)
mvn exec:java -Dexec.mainClass="com.ecommerce.jobs.SilverTransformationJob"
```

### Configuration

Set environment using system property:
```bash
# Development (default)
mvn exec:java -Dexec.mainClass="com.ecommerce.jobs.DataIngestionJob" -Denv=dev

# Production
mvn exec:java -Dexec.mainClass="com.ecommerce.jobs.DataIngestionJob" -Denv=prod
```

---

## ğŸ“ˆ Performance Optimizations

1. **Partitioning Strategy**
   - Silver Orders: Partitioned by `order_year` and `order_month`
   - Improves query performance for time-based analytics

2. **Adaptive Query Execution**
   - Enabled via `spark.sql.adaptive.enabled=true`
   - Dynamically optimizes query plans

3. **Serialization**
   - Using Kryo serializer for better performance
   - Reduces memory footprint

4. **Shuffle Partitions**
   - Optimized for local development (4 partitions)
   - Can be scaled up for production clusters

---

## ğŸ§ª Testing Strategy

### Unit Tests (Coming Soon)
- Schema validation tests
- Transformation logic tests
- Data quality rule tests

### Integration Tests (Coming Soon)
- End-to-end pipeline tests
- Data lineage validation

---

## ğŸ“Š Monitoring & Observability

### Logging Levels
- **DEBUG**: Development troubleshooting
- **INFO**: Normal operations (default)
- **WARN**: Production monitoring
- **ERROR**: Failures and exceptions

### Metrics Tracked
- Total records processed
- Valid vs invalid records
- Duplicates removed
- Processing time
- Error rates

### Log Outputs
- Console logs (development)
- Rolling file logs (`logs/app.log`)
- JSON logs for production (optional)

---

## ğŸ“ Key Design Patterns Implemented

1. **Medallion Architecture** - Multi-layer data lake design
2. **Idempotency** - Safe reruns with `SaveMode.Overwrite`
3. **Dead Letter Queue** - Quarantine bad records for investigation
4. **Schema Evolution** - Type-safe schema definitions
5. **Configuration Management** - Environment-specific configs
6. **Factory Pattern** - SparkSession creation
7. **Result Pattern** - Structured transformation results

---

## ğŸ“ Data Lineage

```
Raw CSV â†’ Bronze (Parquet) â†’ Silver (Cleaned) â†’ Gold (Aggregated)
            â†“                      â†“
         Schema            Data Quality
        Validation          Validation
                                â†“
                          Quarantine (DLQ)
```

---

## ğŸ¤ Contributing

This is a personal portfolio project, but suggestions are welcome!

---

## ğŸ“„ License

This project is for educational and portfolio purposes.

---

## ğŸ“§ Contact

**Your Name**  
Data Engineer | 3 Years Experience  
ğŸ“§ your.email@example.com  
ğŸ’¼ [LinkedIn](your-linkedin)  
ğŸ’» [GitHub](your-github)

---

## ğŸ¯ Project Roadmap

- [x] Phase 1: Bronze Layer (Data Ingestion)
- [x] Phase 2: Silver Layer (Data Transformation)
- [ ] Phase 3: Gold Layer (Analytics)
- [ ] Phase 4: Unit & Integration Tests
- [ ] Phase 5: CI/CD Pipeline
- [ ] Phase 6: Dockerization
- [ ] Phase 7: Monitoring & Alerting
- [ ] Phase 8: Documentation & Portfolio

---

**Built with â¤ï¸ to demonstrate production-grade data engineering skills**