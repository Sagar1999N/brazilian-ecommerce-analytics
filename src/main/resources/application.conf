# Base configuration
app {
  name = "Brazilian E-commerce Analytics"
  version = "1.0.0"
  environment = ${?ENV}
  
  spark {
    master = "local[*]"
    app-name = ${app.name}
    
    # Performance tuning for local development
    config {
      "spark.serializer" = "org.apache.spark.serializer.KryoSerializer"
      "spark.sql.shuffle.partitions" = "4"
      "spark.sql.adaptive.enabled" = "true"
      "spark.sql.legacy.timeParserPolicy" = "LEGACY"
    }
  }
  
  # Data paths
  data {
    base-dir = "data"
    
    raw {
      path = ${app.data.base-dir}"/raw/"
      format = "csv"
    }
    
    staging {
      path = ${app.data.base-dir}"/staging/"
      format = "parquet"
    }
    
    processed {
      path = ${app.data.base-dir}"/processed/"
      format = "parquet"
    }
    
    archive {
      path = ${app.data.base-dir}"/archive/"
      retention-days = 30
    }
  }
  
  # Database configuration
  database {
    postgres {
      host = "localhost"
      port = 5432
      name = "ecommerce"
      user = "ecommerce"
      password = "ecommerce"
      schema = "dw"
      
      jdbc-url = "jdbc:postgresql://"${app.database.postgres.host}":"${app.database.postgres.port}"/"${app.database.postgres.name}
      
      pool {
        max-size = 10
        connection-timeout = 30000
        idle-timeout = 600000
        max-lifetime = 1800000
      }
    }
  }
  
  # Logging configuration
  logging {
    level = "INFO"
    path = "logs/"
    pattern = "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
    
    # Structured logging for production
    json-output = false
  }
  
  # Monitoring
  monitoring {
    metrics {
      enabled = true
      port = 9090
      path = "/metrics"
    }
    
    health-check {
      enabled = true
      interval-seconds = 60
    }
  }
}